# -*- coding: utf-8 -*-
"""Qwen2.5_(3B)-GRPO.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2.5_(3B)-GRPO.ipynb

To run this, press "*Runtime*" and press "*Run all*" on a **free** Tesla T4 Google Colab instance!
<div class="align-center">
<a href="https://unsloth.ai/"><img src="https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png" width="115"></a>
<a href="https://discord.gg/unsloth"><img src="https://github.com/unslothai/unsloth/raw/main/images/Discord button.png" width="145"></a>
<a href="https://docs.unsloth.ai/"><img src="https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true" width="125"></a></a> Join Discord if you need help + ⭐ <i>Star us on <a href="https://github.com/unslothai/unsloth">Github</a> </i> ⭐
</div>

To install Unsloth on your own computer, follow the installation instructions on our Github page [here](https://docs.unsloth.ai/get-started/installing-+-updating).

You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)

### News

**Read our [blog post](https://unsloth.ai/blog/r1-Think) for guidance on how to train Think models.**

Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).

### Installation
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# # Skip restarting message in Colab
# import sys; modules = list(sys.modules.keys())
# for x in modules: sys.modules.pop(x) if "PIL" in x or "google" in x else None
# 
# !pip install unsloth vllm
# !pip install --upgrade pillow
# # If you are running this notebook on local, you need to install `diffusers` too
# # !pip install diffusers
# # Temporarily install a specific TRL nightly version
# !pip install git+https://github.com/huggingface/trl.git@e95f9fb74a3c3647b86f251b7e230ec51c64b72b

"""### Unsloth

Use `PatchFastRL` before all functions to patch GRPO and other RL algorithms!
"""

from unsloth import FastLanguageModel, PatchFastRL
PatchFastRL("GRPO", FastLanguageModel)

"""Load up `Qwen 2.5 3B Instruct`, and set parameters"""

from unsloth import is_bfloat16_supported
import torch
max_seq_length = 2048 # Can increase for longer Think traces
lora_rank = 64 # Larger rank = smarter, but slower

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "/home/jc/mod/qwen2.5-3b-Instruct",
    max_seq_length = max_seq_length,
    load_in_4bit = True, # False for LoRA 16bit
    fast_inference = True, # Enable vLLM fast inference
    max_lora_rank = lora_rank,
    gpu_memory_utilization = 0.75, # Reduce if out of memory
    use_cuda_graph = True,    # 加速
)

model = FastLanguageModel.get_peft_model(
    model,
    r = lora_rank, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128
    target_modules = [
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj",
    ], # Remove QKVO if out of memory
    lora_alpha = lora_rank,
    use_gradient_checkpointing = "unsloth", # Enable long context finetuning
    random_state = 3407,
)

"""### Data Prep
<a name="Data"></a>

We directly leverage [@willccbb](https://gist.github.com/willccbb/4676755236bb08cab5f4e54a0475d6fb) for data prep and all reward functions. You are free to create your own!
"""

import re
from datasets import load_dataset, Dataset
import os
import yaml
import wandb
import resource
import tempfile
import shutil
from datetime import datetime
from contextlib import contextmanager
from transformers.trainer_callback import TrainerCallback

# 加载配置
with open("config.yaml", "r") as f:
    config = yaml.safe_load(f)

# 设置代理
os.environ['HTTPS_PROXY'] = config['proxy']['https']
os.environ['HTTP_PROXY'] = config['proxy']['http']

def set_ulimit():
    try:
        soft, hard = resource.getrlimit(resource.RLIMIT_NOFILE)
        resource.setrlimit(resource.RLIMIT_NOFILE, (hard, hard))
    except Exception as e:
        print(f"设置文件句柄限制失败: {e}")

@contextmanager
def manage_temp_dir():
    original_temp = tempfile.gettempdir()
    try:
        temp_dir = tempfile.mkdtemp(prefix='train_temp_')
        tempfile.tempdir = temp_dir
        yield
    finally:
        tempfile.tempdir = original_temp
        try:
            shutil.rmtree(temp_dir, ignore_errors=True)
        except Exception as e:
            print(f"清理临时目录失败: {e}")

# 系统提示词和数据集准备
SYSTEM_PROMPT = """
按照以下格式进行数学问题的细致推理和解答：
在Think中要理解问题要求并列出已知条件尝试设计解题策略开始逐步计算过程并考虑验证结果
在answer中请详细验证推理结果答案是否准确并且补充说明解法的适用条件或限制查看可能存在的其他解法或变化思考相关的数学概念和注意事项并尝试使用MarkDown格式输出。
请使用中文并且使用以下格式进行输出：
<Think>
...
</Think>

<answer>
...
</answer>
"""

XML_COT_FORMAT = """\
<Think>
{Think}
</Think>
<answer>
{answer}
</answer>
"""

def extract_xml_answer(text: str) -> str:
    answer = text.split("<answer>")[-1]
    answer = answer.split("</answer>")[0]
    return answer.strip()

def extract_hash_answer(text: str) -> str | None:
    if "####" not in text:
        return None
    return text.split("####")[1].strip()

def get_gsm8k_questions(config, split = None) -> Dataset:
    split = split or config['data']['split']
    data = load_dataset(
        config['data']['dataset_name'], 
        config['data']['dataset_config']
    )[split]
    
    data = data.map(lambda x: {
        'prompt': [
            {'role': 'system', 'content': config['data']['system_prompt']},
            {'role': 'user', 'content': x['question']}
        ],
        'answer': extract_hash_answer(x['answer'])
    })
    return data

dataset = get_gsm8k_questions(config)
def think_quality_reward_len_func(prompts, completions, answer, **kwargs) -> list[float]:
    responses = [completion[0]['content'] for completion in completions]
    
    def evaluate_think_length(response):
        think_match = re.search(r'<Think>(.*?)</Think>', response, re.DOTALL)
        if not think_match:
            print("\nThink标签未找到")
            return 0.0
        
        think_content = think_match.group(1).strip()
        content_length = len(think_content)
        
        # 提高各档位的分数
        if content_length < 300:
            return 0
        elif content_length < 400:
            return 1.0
        elif content_length < 600:
            return 1.5        # 从0.5提高到1.0
        elif content_length < 800:
            return 2.0        # 从1.0提高到2.0
        elif content_length < 1000:
            return 3.0 
        elif content_length < 1600:
            return 4.0 
        else:
            return 5.0        # 从2.0提高到4.0
    
    rewards = [evaluate_think_length(r) for r in responses]
    
    # 打印当前Think长度和得分（用于调试）
    think_content = re.search(r'<Think>(.*?)</Think>', responses[0], re.DOTALL)
    if think_content:
        content_length = len(think_content.group(1).strip())
        print(f"\nThink长度: {content_length}")
        print(f"Think长度得分: {rewards[0]:.2f}")
    
    return rewards


def answer_quality_reward_len_func(prompts, completions, answer, **kwargs) -> list[float]:
    responses = [completion[0]['content'] for completion in completions]
    
    def evaluate_answer_length(response):
        answer_match = re.search(r'<answer>(.*?)</answer>', response, re.DOTALL)
        if not answer_match:
            print("\nanswer标签未找到")
            return 0.0
        
        answer_content = answer_match.group(1).strip()
        content_length = len(answer_content)
        
        # 提高各档位的分数
        if content_length < 300:
            return 0
        elif content_length < 400:
            return 1.0
        elif content_length < 600:
            return 1.5        # 从0.5提高到1.0
        elif content_length < 800:
            return 2.0        # 从1.0提高到2.0
        elif content_length < 1000:
            return 3.0 
        elif content_length < 1600:
            return 4.0 
        else:
            return 5.0        # 从2.0提高到4.0
    
    rewards = [evaluate_answer_length(r) for r in responses]
    
    # 打印当前答案长度和得分（用于调试）
    answer_content = re.search(r'<answer>(.*?)</answer>', responses[0], re.DOTALL)
    if answer_content:
        content_length = len(answer_content.group(1).strip())
        print(f"\n答案长度: {content_length}")
        print(f"答案长度得分: {rewards[0]:.2f}")
    
    return rewards
# Reward functions
def correctness_reward_func(prompts, completions, answer, **kwargs) -> list[float]:
    responses = [completion[0]['content'] for completion in completions]
    q = prompts[0][-1]['content']
    extracted_responses = [extract_xml_answer(r) for r in responses]
    
    # 改进打印格式
    print('\n' + '='*50)
    print(f"问题:\n{q}\n")
    print(f"期望答案:\n{answer[0]}\n")
    print(f"模型完整输出:\n{responses[0]}\n")
    
    # 更精确的答案匹配
    def check_answer(response, expected):
        if str(expected).isdigit():
            pattern = r'(?:^|[^\d])' + str(expected) + r'(?:[^\d]|$)'
            return bool(re.search(pattern, response))
        return response.strip() == str(expected)
    
    # 计算答案奖励
    rewards = [2.0 if check_answer(r, answer[0]) else 0.0 for r in extracted_responses]
    
    print(f"\n答案评分: {rewards[0]:.2f}")
    print('='*50 + '\n')
    
    return rewards

def int_reward_func(completions, **kwargs) -> list[float]:
    responses = [completion[0]['content'] for completion in completions]
    extracted_responses = [extract_xml_answer(r) for r in responses]
    return [0.5 if r.isdigit() else 0.0 for r in extracted_responses]

def strict_format_reward_func(completions, **kwargs) -> list[float]:
    """Reward function that checks if the completion has a specific format."""
    pattern = r"^<Think>\n.*?\n</Think>\n<answer>\n.*?\n</answer>\n$"
    responses = [completion[0]["content"] for completion in completions]
    matches = [re.match(pattern, r) for r in responses]
    return [0.5 if match else 0.0 for match in matches]

def soft_format_reward_func(completions, **kwargs) -> list[float]:
    """Reward function that checks if the completion has a specific format."""
    pattern = r"<Think>.*?</Think>\s*<answer>.*?</answer>"
    responses = [completion[0]["content"] for completion in completions]
    matches = [re.match(pattern, r) for r in responses]
    return [0.5 if match else 0.0 for match in matches]
def evaluate_answer_content(text: str) -> float:
    # 提取<answer>标签中的内容
    answer_match = re.search(r'<answer>(.*?)</answer>', text, re.DOTALL)
    if not answer_match:
        return 0.0
    
    answer_content = answer_match.group(1).strip()
    score = 0.0
    
    # 评估答案的完整性和质量
    if len(answer_content.split('\n')) >= 2:  # 至少包含两个段落
        score += 0.2
    if '但是' in answer_content or '然而' in answer_content:  # 包含条件或限制
        score += 0.2
    if '建议' in answer_content or '注意' in answer_content:  # 包含建议或注意事项
        score += 0.2
    if '例如' in answer_content or '比如' in answer_content:  # 包含具体例子
        score += 0.2
    if any(word in answer_content for word in ['可能', '或许', '也许']):  # 考虑其他可能性
        score += 0.2
        
    return score

def evaluate_think_content(text: str) -> float:
    # 提取<Think>标签中的内容
    think_match = re.search(r'<Think>(.*?)</Think>', text, re.DOTALL)
    if not think_match:
        return 0.0
    
    think_content = think_match.group(1).strip()
    score = 0.0
    
    # 评估推理的完整性和质量
    if len(think_content.split('\n')) >= 3:  # 至少有3个推理步骤
        score += 0.3
    if '因为' in think_content or '所以' in think_content:  # 包含因果关系
        score += 0.3
    if '首先' in think_content or '然后' in think_content or '最后' in think_content:  # 有清晰的推理步骤
        score += 0.2
    if '可能' in think_content and '但是' in think_content:  # 考虑多种可能性
        score += 0.2
        
    return score
def think_quality_reward_func(completions, **kwargs) -> list[float]:
    """评估推理过程的质量"""
    responses = [completion[0]['content'] for completion in completions]
    return [evaluate_think_content(r) for r in responses]

def answer_quality_reward_func(completions, **kwargs) -> list[float]:
    """评估答案的完整性和质量"""
    responses = [completion[0]['content'] for completion in completions]
    return [evaluate_answer_content(r) for r in responses]
    

def count_xml(text) -> float:
    count = 0.0
    if text.count("<Think>\n") == 1:
        count += 0.125
    if text.count("\n</Think>\n") == 1:
        count += 0.125
    if text.count("\n<answer>\n") == 1:
        count += 0.125
        count -= len(text.split("\n</answer>\n")[-1])*0.001
    if text.count("\n</answer>") == 1:
        count += 0.125
        count -= (len(text.split("\n</answer>")[-1]) - 1)*0.001
    return count

def xmlcount_reward_func(completions, **kwargs) -> list[float]:
    contents = [completion[0]["content"] for completion in completions]
    return [count_xml(c) for c in contents]

"""<a name="Train"></a>
### Train the model

Now set up GRPO Trainer and all configurations!
"""

# wandb回调
class WandbCallback(TrainerCallback):
    def on_log(self, args, state, control, logs=None, **kwargs):
        if logs:
            wandb.log(logs)
            if "reward" in logs:
                wandb.log({
                    "total_reward": logs["reward"],
                    "reward_std": logs.get("reward_std", 0),
                })

# 训练配置
training_args = GRPOConfig(
    use_vllm = True,
    learning_rate = config['training']['learning_rate'],
    adam_beta1 = 0.9,
    adam_beta2 = 0.99,
    weight_decay = config['training']['weight_decay'],
    warmup_ratio = config['training']['warmup_ratio'],
    lr_scheduler_type = "cosine",
    optim = "adamw_8bit",
    logging_steps = 1,
    bf16 = is_bfloat16_supported(),
    fp16 = not is_bfloat16_supported(),
    per_device_train_batch_size = config['training']['batch_size'],
    gradient_accumulation_steps = config['training']['gradient_accumulation_steps'],
    num_generations = 2,
    max_prompt_length = 256,
    max_completion_length = config['model']['max_seq_length'],
    max_steps = config['training']['max_steps'],
    save_steps = config['training']['save_steps'],
    max_grad_norm = config['training']['max_grad_norm'],
    report_to = "wandb" if config['wandb']['enabled'] else "none",
    output_dir = config['output']['dir'],
)

def main():
    set_ulimit()
    
    if config['wandb']['enabled']:
        wandb.init(
            project=config['wandb']['project'],
            name=f"train_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            config=config
        )

    dataset = get_gsm8k_questions(config)
    
    trainer = GRPOTrainer(
        model = model,
        processing_class = tokenizer,
        reward_funcs = [
            xmlcount_reward_func,
            soft_format_reward_func,
            strict_format_reward_func,
            think_quality_reward_func,
            answer_quality_reward_func,
            correctness_reward_func,
            think_quality_reward_len_func,
            answer_quality_reward_len_func
        ],
        args = training_args,
        train_dataset = dataset,
    )

    trainer.add_callback(WandbCallback())

    with manage_temp_dir():
        try:
            trainer.train()
        finally:
            if config['wandb']['enabled']:
                wandb.finish()

        if torch.cuda.is_available():
            torch.cuda.empty_cache()

    # 保存模型
    if config['output']['save_model']:
        model.save_lora(
            os.path.join(config['output']['dir'], 
            config['output']['model_name'])
        )

if __name__ == "__main__":
    main()

"""<a name="Inference"></a>
### Inference
Now let's try the model we just trained! First, let's first try the model without any GRPO trained:
"""

text = tokenizer.apply_chat_template([
    {"role" : "user", "content" : "草莓里有几个r？"},
], tokenize = False, add_generation_prompt = True)

from vllm import SamplingParams
sampling_params = SamplingParams(
    temperature = 0.8,
    top_p = 0.95,
    max_tokens = 2048,
)
output = model.fast_generate(
    [text],
    sampling_params = sampling_params,
    lora_request = None,
)[0].outputs[0].text

output

"""And now with the LoRA we just trained with GRPO - we first save the LoRA first!"""

model.save_lora("grpo_saved_lora")

"""Now we load the LoRA and test:"""

text = tokenizer.apply_chat_template([
    {"role" : "system", "content" : SYSTEM_PROMPT},
    {"role" : "user", "content" : "How many r's are in strawberry?"},
], tokenize = False, add_generation_prompt = True)

from vllm import SamplingParams
sampling_params = SamplingParams(
    temperature = 0.8,
    top_p = 0.95,
    max_tokens = 8096,
)
output = model.fast_generate(
    text,
    sampling_params = sampling_params,
    lora_request = model.load_lora("grpo_saved_lora"),
)[0].outputs[0].text

output

"""Our Think model is much better - it's not always correct, since we only trained it for an hour or so - it'll be better if we extend the sequence length and train for longer!

<a name="Save"></a>
### Saving to float16 for VLLM

We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens.
"""

# Merge to 16bit
if False: model.save_pretrained_merged("model", tokenizer, save_method = "merged_16bit",)
if False: model.push_to_hub_merged("hf/model", tokenizer, save_method = "merged_16bit", token = "")

# Merge to 4bit
if False: model.save_pretrained_merged("model", tokenizer, save_method = "merged_4bit",)
if False: model.push_to_hub_merged("hf/model", tokenizer, save_method = "merged_4bit", token = "")

# Just LoRA adapters
if False: model.save_pretrained_merged("model", tokenizer, save_method = "lora",)
if False: model.push_to_hub_merged("hf/model", tokenizer, save_method = "lora", token = "")

"""### GGUF / llama.cpp Conversion
To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.

Some supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):
* `q8_0` - Fast conversion. High resource use, but generally acceptable.
* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.
* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K.

[**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://colab.research.google.com/drive/1WZDi7APtQ9VsvOrQSSC5DDtxq159j8iZ?usp=sharing)
"""

# Save to 8bit Q8_0
if False: model.save_pretrained_gguf("model", tokenizer,)
# Remember to go to https://huggingface.co/settings/tokens for a token!
# And change hf to your username!
if False: model.push_to_hub_gguf("hf/model", tokenizer, token = "")

# Save to 16bit GGUF
if False: model.save_pretrained_gguf("model", tokenizer, quantization_method = "f16")
if False: model.push_to_hub_gguf("hf/model", tokenizer, quantization_method = "f16", token = "")

# Save to q4_k_m GGUF
if False: model.save_pretrained_gguf("model", tokenizer, quantization_method = "q4_k_m")
if False: model.push_to_hub_gguf("hf/model", tokenizer, quantization_method = "q4_k_m", token = "")

# Save to multiple GGUF options - much faster if you want multiple!
if False:
    model.push_to_hub_gguf(
        "hf/model", # Change hf to your username!
        tokenizer,
        quantization_method = ["q4_k_m", "q8_0", "q5_k_m",],
        token = "",
    )

"""Now, use the `model-unsloth.gguf` file or `model-unsloth-Q4_K_M.gguf` file in llama.cpp or a UI based system like Jan or Open WebUI. You can install Jan [here](https://github.com/janhq/jan) and Open WebUI [here](https://github.com/open-webui/open-webui)

And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!

Some other links:
1. Llama 3.2 Conversational notebook. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(1B_and_3B)-Conversational.ipynb)
2. Saving finetunes to Ollama. [Free notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)
3. Llama 3.2 Vision finetuning - Radiography use case. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)
6. See notebooks for DPO, ORPO, Continued pretraining, conversational finetuning and more on our [documentation](https://docs.unsloth.ai/get-started/unsloth-notebooks)!

<div class="align-center">
  <a href="https://unsloth.ai"><img src="https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png" width="115"></a>
  <a href="https://discord.gg/unsloth"><img src="https://github.com/unslothai/unsloth/raw/main/images/Discord.png" width="145"></a>
  <a href="https://docs.unsloth.ai/"><img src="https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true" width="125"></a>

  Join Discord if you need help + ⭐️ <i>Star us on <a href="https://github.com/unslothai/unsloth">Github</a> </i> ⭐️
</div>



  Join Discord if you need help + ⭐️ <i>Star us on <a href="https://github.com/unslothai/unsloth">Github</a> </i> ⭐️
</div>

"""
