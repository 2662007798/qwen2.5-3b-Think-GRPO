# -*- coding: utf-8 -*-
"""Qwen2.5_(3B)-GRPO.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2.5_(3B)-GRPO.ipynb

To run this, press "*Runtime*" and press "*Run all*" on a **free** Tesla T4 Google Colab instance!
<div class="align-center">
<a href="https://unsloth.ai/"><img src="https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png" width="115"></a>
<a href="https://discord.gg/unsloth"><img src="https://github.com/unslothai/unsloth/raw/main/images/Discord button.png" width="145"></a>
<a href="https://docs.unsloth.ai/"><img src="https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true" width="125"></a></a> Join Discord if you need help + ⭐ <i>Star us on <a href="https://github.com/unslothai/unsloth">Github</a> </i> ⭐
</div>

To install Unsloth on your own computer, follow the installation instructions on our Github page [here](https://docs.unsloth.ai/get-started/installing-+-updating).

You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)

### News

**Read our [blog post](https://unsloth.ai/blog/r1-Think) for guidance on how to train Think models.**

Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).

### Installation
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# # Skip restarting message in Colab
# import sys; modules = list(sys.modules.keys())
# for x in modules: sys.modules.pop(x) if "PIL" in x or "google" in x else None
# 
# !pip install unsloth vllm
# !pip install --upgrade pillow
# # If you are running this notebook on local, you need to install `diffusers` too
# # !pip install diffusers
# # Temporarily install a specific TRL nightly version
# !pip install git+https://github.com/huggingface/trl.git@e95f9fb74a3c3647b86f251b7e230ec51c64b72b

"""### Unsloth

Use `PatchFastRL` before all functions to patch GRPO and other RL algorithms!
"""

from unsloth import FastLanguageModel, PatchFastRL
PatchFastRL("GRPO", FastLanguageModel)

"""Load up `Qwen 2.5 3B Instruct`, and set parameters"""

from unsloth import is_bfloat16_supported
import torch
max_seq_length = 2048 # Can increase for longer Think traces
lora_rank = 64 # Larger rank = smarter, but slower

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "/home/jc/mod/qwen2.5-3b-Instruct",
    max_seq_length = max_seq_length,
    load_in_4bit = True, # False for LoRA 16bit
    fast_inference = True, # Enable vLLM fast inference
    max_lora_rank = lora_rank,
    gpu_memory_utilization = 0.75, # Reduce if out of memory
)

model = FastLanguageModel.get_peft_model(
    model,
    r = lora_rank, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128
    target_modules = [
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj",
    ], # Remove QKVO if out of memory
    lora_alpha = lora_rank,
    use_gradient_checkpointing = "unsloth", # Enable long context finetuning
    random_state = 3407,
)

"""### Data Prep
<a name="Data"></a>

We directly leverage [@willccbb](https://gist.github.com/willccbb/4676755236bb08cab5f4e54a0475d6fb) for data prep and all reward functions. You are free to create your own!
"""

import re
from datasets import load_dataset, Dataset

# Load and prep dataset
SYSTEM_PROMPT = """
使用MarkDown格式进行输出。
按照以下格式进行数学问题的细致推理和解答：

<Think>
1. 理解问题要求
2. 列出已知条件
3. 设计解题策略
4. 逐步计算过程
5. 验证结果
</Think>

<answer>
1. 一步步验证推理结果答案是否准确，并以MarkDown格式输出
2. 补充说明解法的适用条件或限制
3. 可能存在的其他解法或变化
4. 相关的数学概念和注意事项
</answer>
"""

XML_COT_FORMAT = """\
<Think>
{Think}
</Think>
<answer>
{answer}
</answer>
"""

def extract_xml_answer(text: str) -> str:
    answer = text.split("<answer>")[-1]
    answer = answer.split("</answer>")[0]
    return answer.strip()

def extract_hash_answer(text: str) -> str | None:
    if "####" not in text:
        return None
    return text.split("####")[1].strip()

# uncomment middle messages for 1-shot prompting
def get_gsm8k_questions(split = "train") -> Dataset:
    data = load_dataset('openai/gsm8k', 'main')[split] # type: ignore
    data = data.map(lambda x: { # type: ignore
        'prompt': [
            {'role': 'system', 'content': SYSTEM_PROMPT},
            {'role': 'user', 'content': x['question']}
        ],
        'answer': extract_hash_answer(x['answer'])
    }) # type: ignore
    return data # type: ignore

dataset = get_gsm8k_questions()
def think_quality_reward_len_func(prompts, completions, answer, **kwargs) -> list[float]:
    responses = [completion[0]['content'] for completion in completions]
    
    def evaluate_think_length(response):
        think_match = re.search(r'<Think>(.*?)</Think>', response, re.DOTALL)
        if not think_match:
            print("\nThink标签未找到")
            return 0.0
        
        think_content = think_match.group(1).strip()
        content_length = len(think_content)
        
        # 提高各档位的分数
        if content_length < 300:
            return 0
        elif content_length < 400:
            return 1.0
        elif content_length < 600:
            return 1.5        # 从0.5提高到1.0
        elif content_length < 800:
            return 2.0        # 从1.0提高到2.0
        elif content_length < 1000:
            return 3.0 
        elif content_length < 1600:
            return 4.0 
        else:
            return 5.0        # 从2.0提高到4.0
    
    rewards = [evaluate_think_length(r) for r in responses]
    
    # 打印当前Think长度和得分（用于调试）
    think_content = re.search(r'<Think>(.*?)</Think>', responses[0], re.DOTALL)
    if think_content:
        content_length = len(think_content.group(1).strip())
        print(f"\nThink长度: {content_length}")
        print(f"Think长度得分: {rewards[0]:.2f}")
    
    return rewards


def answer_quality_reward_len_func(prompts, completions, answer, **kwargs) -> list[float]:
    responses = [completion[0]['content'] for completion in completions]
    
    def evaluate_answer_length(response):
        answer_match = re.search(r'<answer>(.*?)</answer>', response, re.DOTALL)
        if not answer_match:
            print("\nanswer标签未找到")
            return 0.0
        
        answer_content = answer_match.group(1).strip()
        content_length = len(answer_content)
        
        # 提高各档位的分数
        if content_length < 300:
            return 0
        elif content_length < 400:
            return 1.0
        elif content_length < 600:
            return 1.5        # 从0.5提高到1.0
        elif content_length < 800:
            return 2.0        # 从1.0提高到2.0
        elif content_length < 1000:
            return 3.0 
        elif content_length < 1600:
            return 4.0 
        else:
            return 5.0        # 从2.0提高到4.0
    
    rewards = [evaluate_answer_length(r) for r in responses]
    
    # 打印当前答案长度和得分（用于调试）
    answer_content = re.search(r'<answer>(.*?)</answer>', responses[0], re.DOTALL)
    if answer_content:
        content_length = len(answer_content.group(1).strip())
        print(f"\n答案长度: {content_length}")
        print(f"答案长度得分: {rewards[0]:.2f}")
    
    return rewards
# Reward functions
def correctness_reward_func(prompts, completions, answer, **kwargs) -> list[float]:
    responses = [completion[0]['content'] for completion in completions]
    q = prompts[0][-1]['content']
    extracted_responses = [extract_xml_answer(r) for r in responses]
    
    # 改进打印格式
    print('\n' + '='*50)
    print(f"问题:\n{q}\n")
    print(f"期望答案:\n{answer[0]}\n")
    print(f"模型完整输出:\n{responses[0]}\n")
    
    # 更精确的答案匹配
    def check_answer(response, expected):
        if str(expected).isdigit():
            pattern = r'(?:^|[^\d])' + str(expected) + r'(?:[^\d]|$)'
            return bool(re.search(pattern, response))
        return response.strip() == str(expected)
    
    # 计算答案奖励
    rewards = [2.0 if check_answer(r, answer[0]) else 0.0 for r in extracted_responses]
    
    print(f"\n答案评分: {rewards[0]:.2f}")
    print('='*50 + '\n')
    
    return rewards

def int_reward_func(completions, **kwargs) -> list[float]:
    responses = [completion[0]['content'] for completion in completions]
    extracted_responses = [extract_xml_answer(r) for r in responses]
    return [0.5 if r.isdigit() else 0.0 for r in extracted_responses]

def strict_format_reward_func(completions, **kwargs) -> list[float]:
    """Reward function that checks if the completion has a specific format."""
    pattern = r"^<Think>\n.*?\n</Think>\n<answer>\n.*?\n</answer>\n$"
    responses = [completion[0]["content"] for completion in completions]
    matches = [re.match(pattern, r) for r in responses]
    return [0.5 if match else 0.0 for match in matches]

def soft_format_reward_func(completions, **kwargs) -> list[float]:
    """Reward function that checks if the completion has a specific format."""
    pattern = r"<Think>.*?</Think>\s*<answer>.*?</answer>"
    responses = [completion[0]["content"] for completion in completions]
    matches = [re.match(pattern, r) for r in responses]
    return [0.5 if match else 0.0 for match in matches]
def evaluate_answer_content(text: str) -> float:
    # 提取<answer>标签中的内容
    answer_match = re.search(r'<answer>(.*?)</answer>', text, re.DOTALL)
    if not answer_match:
        return 0.0
    
    answer_content = answer_match.group(1).strip()
    score = 0.0
    
    # 评估答案的完整性和质量
    if len(answer_content.split('\n')) >= 2:  # 至少包含两个段落
        score += 0.2
    if '但是' in answer_content or '然而' in answer_content:  # 包含条件或限制
        score += 0.2
    if '建议' in answer_content or '注意' in answer_content:  # 包含建议或注意事项
        score += 0.2
    if '例如' in answer_content or '比如' in answer_content:  # 包含具体例子
        score += 0.2
    if any(word in answer_content for word in ['可能', '或许', '也许']):  # 考虑其他可能性
        score += 0.2
        
    return score

def evaluate_think_content(text: str) -> float:
    # 提取<Think>标签中的内容
    think_match = re.search(r'<Think>(.*?)</Think>', text, re.DOTALL)
    if not think_match:
        return 0.0
    
    think_content = think_match.group(1).strip()
    score = 0.0
    
    # 评估推理的完整性和质量
    if len(think_content.split('\n')) >= 3:  # 至少有3个推理步骤
        score += 0.3
    if '因为' in think_content or '所以' in think_content:  # 包含因果关系
        score += 0.3
    if '首先' in think_content or '然后' in think_content or '最后' in think_content:  # 有清晰的推理步骤
        score += 0.2
    if '可能' in think_content and '但是' in think_content:  # 考虑多种可能性
        score += 0.2
        
    return score
def think_quality_reward_func(completions, **kwargs) -> list[float]:
    """评估推理过程的质量"""
    responses = [completion[0]['content'] for completion in completions]
    return [evaluate_think_content(r) for r in responses]

def answer_quality_reward_func(completions, **kwargs) -> list[float]:
    """评估答案的完整性和质量"""
    responses = [completion[0]['content'] for completion in completions]
    return [evaluate_answer_content(r) for r in responses]
    

def count_xml(text) -> float:
    count = 0.0
    if text.count("<Think>\n") == 1:
        count += 0.125
    if text.count("\n</Think>\n") == 1:
        count += 0.125
    if text.count("\n<answer>\n") == 1:
        count += 0.125
        count -= len(text.split("\n</answer>\n")[-1])*0.001
    if text.count("\n</answer>") == 1:
        count += 0.125
        count -= (len(text.split("\n</answer>")[-1]) - 1)*0.001
    return count

def xmlcount_reward_func(completions, **kwargs) -> list[float]:
    contents = [completion[0]["content"] for completion in completions]
    return [count_xml(c) for c in contents]

"""<a name="Train"></a>
### Train the model

Now set up GRPO Trainer and all configurations!
"""

import wandb
from datetime import datetime
from transformers.trainer_callback import TrainerCallback

# 设置环境变量
import os
import resource
import shutil
import tempfile
from contextlib import contextmanager

# 设置文件句柄限制
def set_ulimit():
    try:
        # 获取当前系统的限制
        soft, hard = resource.getrlimit(resource.RLIMIT_NOFILE)
        # 设置为最大值
        resource.setrlimit(resource.RLIMIT_NOFILE, (hard, hard))
        print(f"设置文件句柄限制为: {hard}")
    except Exception as e:
        print(f"设置文件句柄限制失败: {e}")

@contextmanager
def manage_temp_dir():
    original_temp = tempfile.gettempdir()
    try:
        # 创建新的临时目录
        temp_dir = tempfile.mkdtemp(prefix='train_temp_')
        tempfile.tempdir = temp_dir
        yield
    finally:
        # 恢复原始临时目录
        tempfile.tempdir = original_temp
        try:
            shutil.rmtree(temp_dir, ignore_errors=True)
        except Exception as e:
            print(f"清理临时目录失败: {e}")

# 在程序开始时设置限制
set_ulimit()

# 设置环境变量
os.environ['HTTPS_PROXY'] = 'http://10.0.1.31:7890'
os.environ['HTTP_PROXY'] = 'http://10.0.1.31:7890'

# wandb初始化
wandb.init(
    project="logic-rl",
    name=f"train_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
    config={
        "model": "qwen2.5-3b-Instruct",
        "max_seq_length": max_seq_length,
        "lora_rank": lora_rank,
        "learning_rate": 1e-6,
        "batch_size": 1,
        "gradient_accumulation_steps": 4,
    }
)

# 自定义wandb回调
class WandbCallback(TrainerCallback):
    def on_train_begin(self, args, state, control, **kwargs):
        pass
        
    def on_train_end(self, args, state, control, **kwargs):
        pass
        
    def on_log(self, args, state, control, logs=None, **kwargs):
        if logs:
            # 记录所有指标
            wandb.log(logs)
            
            # 添加自定义指标
            if "reward" in logs:
                wandb.log({
                    "total_reward": logs["reward"],
                    "reward_std": logs.get("reward_std", 0),
                })

from trl import GRPOConfig, GRPOTrainer
training_args = GRPOConfig(
    use_vllm = True, # use vLLM for fast inference!
    learning_rate = 1e-6,
    adam_beta1 = 0.9,
    adam_beta2 = 0.99,
    weight_decay = 0.1,
    warmup_ratio = 0.1,
    lr_scheduler_type = "cosine",
    optim = "adamw_8bit",
    logging_steps = 1,
    bf16 = is_bfloat16_supported(),
    fp16 = not is_bfloat16_supported(),
    per_device_train_batch_size = 1,
    gradient_accumulation_steps = 1, # Increase to 4 for smoother training
    num_generations = 2,
    max_prompt_length = 256,
    max_completion_length = 2048,
    max_steps = 1000,
    save_steps = 5,
    max_grad_norm = 0.1,
    report_to = "wandb",  # 启用wandb记录
    output_dir = "outputs",
)

"""And let's run the trainer! If you scroll up, you'll see a table of rewards. The goal is to see the `reward` column increase!

You might have to wait 150 to 200 steps for any action. You'll probably get 0 reward for the first 100 steps. Please be patient!

| Step | Training Loss | reward    | reward_std | completion_length | kl       |
|------|---------------|-----------|------------|-------------------|----------|
| 1    | 0.000000      | 0.125000  | 0.000000   | 200.000000        | 0.000000 |
| 2    | 0.000000      | 0.072375  | 0.248112   | 200.000000        | 0.000000 |
| 3    | 0.000000      | -0.079000 | 0.163776   | 182.500000        | 0.000005 |

"""

trainer = GRPOTrainer(
    model = model,
    processing_class = tokenizer,
    reward_funcs = [
        xmlcount_reward_func,
        soft_format_reward_func,
        strict_format_reward_func,
        # int_reward_func,  移除只要纯数字的答案
        think_quality_reward_func,   # 添加思考质量奖励函数
        answer_quality_reward_func,  # 添加答案质量奖励
        correctness_reward_func,
        think_quality_reward_len_func, #思考长度奖励函数
        answer_quality_reward_len_func #答案长度奖励函数
    ],
    args = training_args,
    train_dataset = dataset,
)

# 添加wandb回调
trainer.add_callback(WandbCallback())

# 在训练代码周围使用临时目录管理
with manage_temp_dir():
    try:
        trainer.train()
    finally:
        wandb.finish()

    # 清理CUDA缓存
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

"""<a name="Inference"></a>
### Inference
Now let's try the model we just trained! First, let's first try the model without any GRPO trained:
"""

text = tokenizer.apply_chat_template([
    {"role" : "user", "content" : "草莓里有几个r？"},
], tokenize = False, add_generation_prompt = True)

from vllm import SamplingParams
sampling_params = SamplingParams(
    temperature = 0.8,
    top_p = 0.95,
    max_tokens = 2048,
)
output = model.fast_generate(
    [text],
    sampling_params = sampling_params,
    lora_request = None,
)[0].outputs[0].text

output

"""And now with the LoRA we just trained with GRPO - we first save the LoRA first!"""

model.save_lora("grpo_saved_lora")

"""Now we load the LoRA and test:"""

text = tokenizer.apply_chat_template([
    {"role" : "system", "content" : SYSTEM_PROMPT},
    {"role" : "user", "content" : "How many r's are in strawberry?"},
], tokenize = False, add_generation_prompt = True)

from vllm import SamplingParams
sampling_params = SamplingParams(
    temperature = 0.8,
    top_p = 0.95,
    max_tokens = 8096,
)
output = model.fast_generate(
    text,
    sampling_params = sampling_params,
    lora_request = model.load_lora("grpo_saved_lora"),
)[0].outputs[0].text

output

"""Our Think model is much better - it's not always correct, since we only trained it for an hour or so - it'll be better if we extend the sequence length and train for longer!

<a name="Save"></a>
### Saving to float16 for VLLM

We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens.
"""

# Merge to 16bit
if False: model.save_pretrained_merged("model", tokenizer, save_method = "merged_16bit",)
if False: model.push_to_hub_merged("hf/model", tokenizer, save_method = "merged_16bit", token = "")

# Merge to 4bit
if False: model.save_pretrained_merged("model", tokenizer, save_method = "merged_4bit",)
if False: model.push_to_hub_merged("hf/model", tokenizer, save_method = "merged_4bit", token = "")

# Just LoRA adapters
if False: model.save_pretrained_merged("model", tokenizer, save_method = "lora",)
if False: model.push_to_hub_merged("hf/model", tokenizer, save_method = "lora", token = "")

"""### GGUF / llama.cpp Conversion
To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.

Some supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):
* `q8_0` - Fast conversion. High resource use, but generally acceptable.
* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.
* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K.

[**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://colab.research.google.com/drive/1WZDi7APtQ9VsvOrQSSC5DDtxq159j8iZ?usp=sharing)
"""

# Save to 8bit Q8_0
if False: model.save_pretrained_gguf("model", tokenizer,)
# Remember to go to https://huggingface.co/settings/tokens for a token!
# And change hf to your username!
if False: model.push_to_hub_gguf("hf/model", tokenizer, token = "")

# Save to 16bit GGUF
if False: model.save_pretrained_gguf("model", tokenizer, quantization_method = "f16")
if False: model.push_to_hub_gguf("hf/model", tokenizer, quantization_method = "f16", token = "")

# Save to q4_k_m GGUF
if False: model.save_pretrained_gguf("model", tokenizer, quantization_method = "q4_k_m")
if False: model.push_to_hub_gguf("hf/model", tokenizer, quantization_method = "q4_k_m", token = "")

# Save to multiple GGUF options - much faster if you want multiple!
if False:
    model.push_to_hub_gguf(
        "hf/model", # Change hf to your username!
        tokenizer,
        quantization_method = ["q4_k_m", "q8_0", "q5_k_m",],
        token = "",
    )

"""Now, use the `model-unsloth.gguf` file or `model-unsloth-Q4_K_M.gguf` file in llama.cpp or a UI based system like Jan or Open WebUI. You can install Jan [here](https://github.com/janhq/jan) and Open WebUI [here](https://github.com/open-webui/open-webui)

And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!

Some other links:
1. Llama 3.2 Conversational notebook. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(1B_and_3B)-Conversational.ipynb)
2. Saving finetunes to Ollama. [Free notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)
3. Llama 3.2 Vision finetuning - Radiography use case. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)
6. See notebooks for DPO, ORPO, Continued pretraining, conversational finetuning and more on our [documentation](https://docs.unsloth.ai/get-started/unsloth-notebooks)!

<div class="align-center">
  <a href="https://unsloth.ai"><img src="https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png" width="115"></a>
  <a href="https://discord.gg/unsloth"><img src="https://github.com/unslothai/unsloth/raw/main/images/Discord.png" width="145"></a>
  <a href="https://docs.unsloth.ai/"><img src="https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true" width="125"></a>

  Join Discord if you need help + ⭐️ <i>Star us on <a href="https://github.com/unslothai/unsloth">Github</a> </i> ⭐️
</div>



  Join Discord if you need help + ⭐️ <i>Star us on <a href="https://github.com/unslothai/unsloth">Github</a> </i> ⭐️
</div>

"""
